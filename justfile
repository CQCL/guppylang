set shell := ["bash", "-uc"]

# List the available commands
help:
    @just --list --justfile {{justfile()}}

# Prepare the environment for development, installing all the dependencies and
# setting up the pre-commit hooks.
setup:
    uv sync
    [[ -n "${JUST_INHIBIT_GIT_HOOKS:-}" ]] || uv run pre-commit install -t pre-commit

# Prepare the environment for development, including the extra dependency groups.
setup-extras:
    uv sync --extra pytket --inexact

# Run the pre-commit checks.
check:
    uv run pre-commit run --all-files


# Run the tests.
test *PYTEST_FLAGS:
    uv run pytest -n auto {{PYTEST_FLAGS}}

# Export the integration test cases to a directory.
export-integration-tests directory="guppy-exports":
    uv run pytest --export-test-cases="{{ directory }}"

# Auto-fix all clippy warnings.
fix:
    uv run ruff check --fix guppylang

# Format the code.
format:
    uv run ruff format guppylang

# Generate a test coverage report.
coverage:
    uv run pytest --cov=./ --cov-report=html

# Generate the documentation.
build-docs:
    cd docs && ./build.sh

# Serve sphinx docs locally (needs npm installed)
serve-docs: build-docs
    npm exec serve docs/build/api-docs

# Remove files generated by the sphinx build
clean-docs:
    rm -rf docs/build
    rm -rf docs/api-docs/generated

# Package the code and store the wheels in the dist/ directory.
build-wheels:
    uvx --from build pyproject-build --installer uv

# Run benchmarks using pytest-benchmark.
bench *PYTEST_FLAGS:
    uv run pytest --benchmark-only {{PYTEST_FLAGS}}

# Run benchmarks and save JSON data to path/name.json.
bench_save path name:
    uv run pytest --benchmark-only --benchmark-storage={{path}} --benchmark-save={{name}}


NOW := `date +%s%n | tr -d '\n'`
BENCHER_PROJECT := "guppylang-benchmarks"

# Run benchmarks and upload the results using bencher_cli. Note: Needs the BENCHER_API_TOKEN env variable.
bench_upload *BENCHER_FLAGS:
    uv run pytest --benchmark-only --benchmark-json="{{NOW}}-pytest-benchmark.json"
    bencher run \
            --adapter python_pytest \
            --file "{{NOW}}-pytest-benchmark.json" \
            --project {{BENCHER_PROJECT}} \
            --quiet \
            {{BENCHER_FLAGS}}
    uv run python tests/bencher.py "{{NOW}}-pytest-benchmark.json" "{{NOW}}-bencher.json"
    bencher run \
            --file "{{NOW}}-bencher.json" \
            --adapter json \
            --project {{BENCHER_PROJECT}} \
            --quiet \
            {{BENCHER_FLAGS}}

# Run benchmarks and compare the results using bencher_cli. Note: Needs the BENCHER_API_TOKEN env variable.
bench_compare *BENCHER_FLAGS:
    uv run pytest --benchmark-only --benchmark-json="{{NOW}}-pytest-benchmark.json"
    bencher run \
            --project {{BENCHER_PROJECT}} \
            --adapter python_pytest \
            --file "{{NOW}}-pytest-benchmark.json" \
            --average median \
            --threshold-measure latency \
            --threshold-test percentage \
            --threshold-upper-boundary 0.05 \
            --err \
            --quiet \
            {{BENCHER_FLAGS}}
    uv run python tests/bencher.py "{{NOW}}-pytest-benchmark.json" "{{NOW}}-bencher.json"
    bencher run \
            --project {{BENCHER_PROJECT}} \
            --adapter json \
            --file "{{NOW}}-bencher.json" \
            --average median \
            --threshold-measure hugr_bytes \
            --threshold-test percentage \
            --threshold-upper-boundary 0.01 \
            --threshold-measure hugr_nodes \
            --threshold-test percentage \
            --threshold-upper-boundary 0.01 \
            --err \
            --quiet \
            {{BENCHER_FLAGS}}

